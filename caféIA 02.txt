Les 4 modes LightRAG ‚Äî ce qu'ils font vraiment
naive ‚Äî RAG classique pur. Embedding query ‚Üí cosine similarity ‚Üí top-k chunks ‚Üí LLM. Pas de graph, pas de multi-hop. Rapide, mais √©choue d√®s que la r√©ponse n√©cessite de croiser deux entit√©s distinctes. Utile en d√©mo pour montrer pourquoi le graph est n√©cessaire.
local ‚Äî Requ√™te centr√©e sur les entit√©s et leurs relations imm√©diates (1 hop). "Qui est Marc Dupont ?" ‚Üí extrait l'entit√© + ses voisins directs dans le graph. Plus pr√©cis que naive sur les questions factuelles directes.
global ‚Äî Parcourt les patterns transversaux du graph entier. Utile pour des questions abstraites : "Quelles sont les tendances du secteur ?", "Quels sont les risques syst√©miques ?". Plus lent, contexte plus large.
hybrid ‚Äî local + global combin√©s. C'est le seul mode vraiment multi-hop : traverse le graph en suivant les relations (entit√© ‚Üí certification ‚Üí technicien ‚Üí r√©gion). C'est ce qui r√©pond correctement √† "Quel technicien certifi√© PV est dispo en Occitanie pour le SolarMax 3000 ?". Recommand√© par d√©faut.



-------

Ton qwen2.5:14b est en-dessous des specs officielles. 14B au lieu de 32B minimum ‚Äî l'extraction d'entit√©s/relations sera incompl√®te ou approximative.
Meilleurs couples pour ton serveur Mensaflow :
Priorit√©LLMEmbeddingDimNoteü•á Qualit√©qwen2.5:32bbge-m31024Specs officielles LightRAGü•à √âquilibreqwen2.5:14bmxbai-embed-large1024Ton setup actuel + meilleur embeddingü•â D√©mo rapideqwen2.5:7bnomic-embed-text768Vitesse avant qualit√©
mxbai-embed-large surpasse text-embedding-3-large d'OpenAI tout en √©tant significativement plus petit, et nomic-embed-text surpasse text-embedding-ada-002 sur les t√¢ches long-contexte. TigerData

--------

1. CHUNKING          ~1 sec    ‚Äî d√©coupe le texte en chunks (1200 tokens)
                                 classique, rien de sp√©cial

2. ENTITY EXTRACTION  >> 80%   ‚Äî LLM appel√© sur CHAQUE chunk pour extraire :
   (la partie lente)              ‚Ä¢ entit√©s  (Marc Dupont, SolarMax 3000, Occitanie)
                                  ‚Ä¢ relations (Marc certifi√© PV, SolarMax vendu par...)
                                  ‚Ä¢ attributs (r√©gion, certification, disponibilit√©)
                                  C'est une inf√©rence LLM compl√®te par chunk

3. GRAPH CONSTRUCTION ~2 sec   ‚Äî Neo4j / NanoVectorDB :
                                  ‚Ä¢ noeuds cr√©√©s pour chaque entit√© unique
                                  ‚Ä¢ ar√™tes cr√©√©es pour chaque relation
                                  ‚Ä¢ d√©duplication + merge si entit√© d√©j√† connue

4. DUAL INDEXING      ~3 sec   ‚Äî stocke DEUX repr√©sentations :
                                  ‚Ä¢ vecteurs (pour le nearest-neighbor search)
                                  ‚Ä¢ graph (pour le multi-hop traversal)
                                  
---
qwen2.5:14b fait ~3-4 appels LLM par chunk pour l'extraction. Ton PDF de 275KB = ~20-25 chunks = 80-100 inf√©rences LLM. C'est structurellement incompressible ‚Äî c'est le prix du graph.
La bonne nouvelle : c'est one-time. Le graph est persist√© dans ./storage. Au prochain lancement, z√©ro re-indexation. La query hybride multi-hop, elle, prend 5-15 secondes.

---
Option 2 ‚Äî Mistral API pour la query (meilleure) :
LightRAG permet des mod√®les diff√©rents pour indexation vs query. Indexation en local (14b, lent mais one-time), query via Mistral API (100x plus rapide) :


---------- MISTRAL KEY
https://console.mistral.ai/home?workspace_dialog=apiKeys

REDACTED

---------- QUERY BALANCE
lean     ‚Üí top_k=3,  tokens 1500/1000/1000  (~4k total)
balanced ‚Üí top_k=5,  tokens 2000/1500/1500  (~6k total)
full     ‚Üí top_k=10, tokens 4000/4000/4000  (~15k = d√©faut LightRAG)

----- BUG pour QUERY...

Deux bugs corrig√©s :
Bug 1 ‚Äî "multiple values for argument 'model'"
La signature r√©elle est openai_complete_if_cache(model, prompt, ...). LightRAG passe aussi model en kwarg via llm_model_name. Le wrapper recevait donc model en double. Fix : kwargs.pop("model", None) avant l'appel + passer _qmodel en 1er arg positionnel.
Bug 2 ‚Äî WARNING rerank
enable_rerank=True par d√©faut dans ta version. Aucun rerank model configur√© ‚Üí warning √† chaque query. D√©sactiv√© directement dans QueryParam.
Ctrl+C ‚Üí streamlit run app.py ‚Äî le cache @st.cache_resource sera invalid√© automatiquement car la fonction get_query_rag a chang√©.

