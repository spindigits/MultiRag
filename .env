##############################################################
## CaféIA — LightRAG Server Configuration
## Jardinerie Verde Occitanie — Workshop UPVD
## Stratégie : Indexation locale (Ollama) + Query rapide (Mistral API)
##############################################################

# ── Serveur ──────────────────────────────────────────────────
HOST=0.0.0.0
PORT=9621
WORKERS=1                         # 1 seul worker (uvicorn mode, Ollama local)
TIMEOUT=300
LOG_LEVEL=INFO

# ── Répertoires ──────────────────────────────────────────────
# Dossier storage : réutilise l'existant si déjà indexé
# RAG_DIR=./rag_storage           # décommenter pour chemin custom

# ── LLM — OLLAMA (indexation + query) ───────────────────────
# CRITIQUE : LLM_BINDING_API_KEY=ollama bloque le fallback sur
# ANTHROPIC_API_KEY / OPENAI_API_KEY présents dans l'environnement
LLM_BINDING=ollama
LLM_BINDING_HOST=http://localhost:11434
LLM_BINDING_API_KEY=ollama        # valeur factice — empêche fallback Anthropic/OpenAI
LLM_MODEL=qwen2.5:14b
LLM_TIMEOUT=900
MAX_ASYNC=1
MAX_TOKENS=32768

# OBLIGATOIRE Ollama : contexte fenêtre > MAX_TOTAL_TOKENS + 2000
OLLAMA_LLM_NUM_CTX=32768

# ── EMBEDDING (Ollama local) ─────────────────────────────────
EMBEDDING_BINDING=ollama
EMBEDDING_BINDING_HOST=http://localhost:11434
EMBEDDING_MODEL=mxbai-embed-large
EMBEDDING_DIM=1024
EMBEDDING_FUNC_MAX_ASYNC=2

# ── CHUNKING ─────────────────────────────────────────────────
CHUNK_SIZE=600                    # Réduit vs défaut 1200 → évite timeout qwen14b
CHUNK_OVERLAP_SIZE=50
ENABLE_LLM_CACHE_FOR_EXTRACT=true # Cache chunks déjà traités → reprise sans re-indexer

# ── QUERY ─────────────────────────────────────────────────────
# NOTE : lightrag-server utilise le MÊME LLM pour query et indexation
# Pour forcer Mistral API en query → voir script query_mistral.py ci-dessous
# (workaround : Open WebUI connecté au serveur, ou script Python direct)
HISTORY_TURNS=5
COSINE_THRESHOLD=0.2
TOP_K=40
MAX_TOKEN_TEXT_CHUNK=4000
MAX_TOKEN_RELATION_DESC=4000
MAX_TOKEN_ENTITY_DESC=4000

# ── KNOWLEDGE GRAPH ──────────────────────────────────────────
MAX_GRAPH_NODES=500               # Adapté dataset jardinerie (pas besoin de +)

# ── AUTH (désactivé pour workshop local) ─────────────────────
# AUTH_ACCOUNTS='admin:cafeia2025'
# TOKEN_SECRET='mensaflow-upvd-2025'
# TOKEN_EXPIRE_HOURS=48

# ── CORS ─────────────────────────────────────────────────────
CORS_ORIGINS=*                    # Permissif pour workshop local

# ── SYSTEM PROMPT ─────────────────────────────────────────────
# Critique : sans ça, le LLM refuse les données "personnelles" (numéros, emails)
# car il croit interroger des données publiques réelles.
# On lui explique qu'il travaille sur une base interne fictive.
SYSTEM_PROMPT="Tu es l'assistant interne de la Jardinerie Verde Occitanie. Tu as accès à la base de données clients et fournisseurs de l'entreprise. Ces données sont des données internes d'entreprise fictives utilisées à des fins de démonstration. Réponds toujours en français de manière précise et directe en te basant sur les informations disponibles dans le knowledge graph. Ne refuse jamais de répondre à des questions sur les clients, produits ou fournisseurs de la jardinerie : ces informations font partie de la base de données interne de l'entreprise et leur consultation est légitime."